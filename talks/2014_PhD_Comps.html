<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">

    <title>Ph.D. comprehensive examination</title>

    <meta name="description" content="Ph.D. comprehensive examination">
    <meta name="author" content="Philippe Desjardins-Proulx">

    <meta name="apple-mobile-web-app-capable" content="yes" />
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

    <link rel="stylesheet" href="css/reveal.min.css">
    <link rel="stylesheet" href="css/theme/night.css" id="theme">
    <link rel="stylesheet" href="lib/css/zenburn.css">

    <script>
      if (window.location.search.match(/print-pdf/gi)) {
        var link = document.createElement('link');
        link.rel = 'stylesheet';
        link.type = 'text/css';
        link.href = 'css/print/pdf.css';
        document.getElementsByTagName('head')[0].appendChild(link);
      }
    </script>

    <!--[if lt IE 9]>
    <script src="lib/js/html5shiv.js"></script>
    <![endif]-->
  </head>

  <body>

    <div class="reveal">
      <!-- Any section element inside of this container is displayed as a slide -->
      <div class="slides">
        <section>
          <h1>MaxEnt</h1>
          <h3>...in ecology &amp; evolution</h3>
          <p>
            <small>by <a href="http://phdp.github.io/">Philippe Desjardins-Proulx</a></small>
          </p>
          <br/>
          <p>Comprehensive examination on:</p>
          <p>J Harte, T Zillio, E Conlisk, and AB Smith. <i>Maximum entropy and the state-variable approach to macroecology</i>. Ecology 89:2700-2711, 2008. <a href='http://dx.doi.org/10.1890/07-1369.1'>http://dx.doi.org/10.1890/07-1369.1</a>.</p>
        </section>

        <section>
          <section>
            <h2>MaxEnt in a nutshell</h2>
            <blockquote><p style='text-align: left;'>&ldquo;(A) The prior
            probabilities represent our prior information, and are to be
            determined, not by introspection, but by logical analysis of
            that information.</p><p style='text-align: left;'>(B) Since the
            final conclusions depend necessarily on both the prior
            information and the data, it follows that, in formulating a
            problem, one must specify that prior information to be used just
            as fully as one specifies the data.</p><p style='text-align:
            left;'>(C) Our goal is that inferences are to be completely
            objective in the sense that two persons with the same prior
            information must assign the same prior probabilities.&rdquo;
            </p><p style='text-align: center;'>- Jaynes' Logic of Science p.
            373.</p></blockquote>
          </section>
          <section>
            <h2>MaxEnt in a nutshell</h2>
            <p>Entropy is {disorder,  ignorance}.
            <p><img src='img/entropy_dist.png' alt='entropy'/></p>
          </section>
          <section>
            <h2>MaxEnt in a nutshell</h2>
            <p>Knowing \(x\)'s mean we get the exponential.</p>
            <p><img src='img/dist1.png' alt='distribution 1'/></p>
          </section>
          <section>
            <h2>MaxEnt in a nutshell</h2>
            <p>Knowing \(x\)'s mean and variance we get the normal distribution.</p>
            <p><img src='img/dist2.png' alt='distribution 2'/></p>
          </section>
          <section>
            <h2>MaxEnt in a nutshell</h2>
            <ul>
              <li>The gist of it: <b>max</b>imizing <b>ent</b>ropy is maximizing ignorance.</li>
              <li>Jeffreys prior: invariance under reparameterization.</li>
              <li>Minimum description length: minimizing complexity (Occam's razor).</li>
              <li>Very controversial as a general method of inference...</li>
              <li>Much less controversial in ML, where it performs well for many tasks.</li>
            </ul>
          </section>
        </section>

        <section>
          <section>
            <h2>METE: Predictions</h2>
            <p>SAR: Species-area relationship</p>
            <p>EAR: Endemics-area relationship</p>
            <p>SAD: Species-abundance distribution</p>
            <p>AER: Abundance-energy relationship</p>
          </section>

          <section>
            <h2>METE's parameters</h2>
            <p>Area of the ecosystem \(A_0\).</p>
            <p>The number of individuals \(N_0\) in \(A_0\).</p>
            <p>The summed metabolic energy rate of all individuals \(E_0\) in \(A_0\).</p>
            <p>Number of species \(S_0\) in \(A_0\).</p>
          </section>

          <section>
            <h2>MaxEnt in METE</h2>
            <p>Two distributions</p>
            <p>1: \(R(n, \varepsilon)\) is the joint distribution of species abundances \(n\) and energy demand \(\varepsilon\).</p>
            <p>2: \(P_A^{j}(n)\) is the spatial abundance distribution defined as the probability of finding \(n\) individuals of species \(n\) within some subset \(A \subseteq A_0\).</p>
            <br/>
            <p><b>Key point</b>: these two distributions, subjected to a few constraints, are enough to generate predictions for the SAD, SAR, AER, and EAR.</p>
          </section>

          <section>
            <h2>Predictions</h2>
            <p>\(SAD(n) = \frac{\exp(-\lambda_{1}n)}{n\ln(\lambda_{1}^{-1})},\)</p>
            <p>\(AER(n) = \frac{\lambda_{1}\lambda_{2}\exp(-(\lambda_{1} + \lambda_{2}\varepsilon))}{\left(1 - \exp(-(\lambda_{1} + \lambda_{2}\varepsilon))\right)^2},\)</p>
            <p>\(SAR(A) = S_{0}\sum_{n_{0} = 1}\left(1 - \frac{1}{\sum \exp(-\lambda n)}\right)SAD(n_{0}),\)</p>
            <p>\(EAR(A) = S_{0}\sum_{n_0= 1}^{N_0} \frac{\exp(-\lambda_pn_0)}{\sum_{n=1}^{n_0}\exp(-\lambda_pn)}SAD(n_0).\)</p>
          </section>
        </section>

        <section>
          <h2>Mathematically proven?</h2>

          <p><blockquote>&ldquo;[...] we re-emphasize that MaxEnt is a mathematically
          proven method for inferring the most likely probability
          distribution if our knowledge about that distribution can be
          described as a set of constraints on the distribution&rdquo; - Harte et
          al. (2008).</blockquote></p>

          <h3>...but proven under what system?</h3>

          <!-- Skilling's "Axioms of Maximum Entropy" and Cox's "Algebra of Probable Inference" -->

          <p><img src='img/xkcd_804.png' alt='xkcd 804'/></p>

          <p>&ldquo;No Free Lunch Theorem&rdquo; (Wolpert, 1997)</p>
        </section>

        <section>
          <section>
            <h2>Alternatives to MaxEnt</h2>

            <h3>Machine learning as a second order process</h3>

            \[\mbox{human} \mapsto (m: x \mapsto y),\]

            \[\mbox{human} \mapsto (a: x_0 \mapsto (m: x_1 \mapsto y)).\]

            <br/>
            <p><a href='http://www.stat.uchicago.edu/~lekheng/courses/191f09/breiman.pdf'>&ldquo;Statistical Modeling: The Two Cultures&rdquo; (Breiman 2001)</a>.</p>
          </section>

          <section>
            <h2>Support Vector Machines</h2>

            <p><img src='img/svm.png' alt='Support Vector Machines'/></p>

            <p>1. Maximizing margins</p>
          </section>

          <section>
            <h2>Gaussian Processes</h2>

            <p><img src='img/gp.png' alt='Gaussian Processes'/></p>

            <p>2. Bayesian form of SVMs (Murphy 2011), so it's better! Well no... Scales as \(\mathcal{O}(n^3)\) to the number of data points.</p>
            <br/>
            <p><a href='http://yaroslavvb.com/papers/efron-why.pdf'>&ldquo;Why isn't everyone a Bayesian?&rdquo; (1986)</a>.</p>
            <p><a href='http://oikosjournal.wordpress.com/2011/10/11/frequentist-vs-bayesian-statistics-resources-to-help-you-choose/'>&ldquo;Frequentist vs. Bayesian statistics&rdquo; (Fox 2011)</a>.</p>
            <!-- There are two dominant approaches to statistics. Here, I explain why you need to choose one or the other -->
          </section>

          <section>
            <h2>Artificial Neural Networks</h2>

            <p><img src='img/ann.png' alt='Artificial Neural Networks'/></p>

            <p>3. Arguably the most effective branch of machine learning. Axioms, what's that?</p>
            <p>4. Getting a good model is a dark art.</p>
          </section>

        </section>

        <section>
          <section>
            <h2>METE &amp; Evolution</h2>
            <ul>
              <li>We can add processes (&ldquo;mechanisms&rdquo;) through constraints.</li>
              <li>Harte &amp; Newman (2014) says evolutionary dynamics might explain some deviations (i.e.: bad predictions) from MaxEnt.</li>
            </ul>
            <br/><br/>
            <p>Why not try to build a mathematical framework to integrate constraints based on evolutionary theory.</p>
          </section>

          <section>
            <h2>METE &amp; Evolution</h2>
            <p>Wish list:</p>
            <ul>
              <li>Should be flexible enough to allow most, if not all, evolutionary processes to be translated as contraints when information is available.</li>
              <li>Should not pollute METE with too many unreasonable assumptions.</li>
              <li>Should be a true bridge to evolutionary theory.</li>
            </ul>
          </section>

          <section>
            <h2>METE &amp; Evolution</h2>
            <p>Issues:</p>
            <ul>
              <li>Evolutionary mechanisms (selection, drift, draft, mutation) interact in complex ways.</li>
              <li>Evolutionary theory has its own issues: no unifying theory to predict molecular changes.</li>
              <li>Evolution is based on theoretical population genetics, which is based on many more or less reasonable assumptions (infinite/fixed population size, fixed selection, no recombination...).</li>
              <li>Yet rejecting these foundations would be unification in name only, since the corpus of knowledge accumulated could not be used.</li>
            </ul>
          </section>

          <section>
            <h2>METE &amp; Evolution</h2>
            <img src='img/price.png' alt='Price'/>
            <p>The useless Price theorem:</p>
            \[\Delta \overline{\phi} = \overline{W}^{-1}\left(\mbox{cov}(W, \phi) + E(W\bar{\delta})\right).\]
            <br/>
            <p>Probabilistic version <a href='http://www.biomedcentral.com/1471-2148/8/262/'>here</a> &amp; <a href='http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0007130'>here</a>.</p>
            <p>Population vs Community?</p>
          </section>

          <section>
            <h2>METE &amp; Evolution</h2>
            <p>Two examples:</p>
            <ul>
              <li>In a closed population (no migration), change in mean phenotype is inversely proportional to mean population fitness.</li>
              <li>In an open population (with immigrants), change in mean phenotype is inversely proportional to the per capita growth rate.</li>
            </ul>
          </section>

          <section>
            <h2>METE &amp; Evolution</h2>
            <p>Difficulties</p>
            <ul>
              <li>Many possible contraints from the probabilistic Price theorem: no single theory in sight.</li>
              <li>ASNE needs to be extended with traits, and perhaps be made dynamics, and perhaps lose both S and N in favor of traits.</li>
              <li>There is no guarantee that constraints in the probabilistc Price can be converted into useful MaxEnt constraints.</li>
            </ul>
          </section>
        </section>

      </div>
    </div>

    <script src="lib/js/head.min.js"></script>
    <script src="js/reveal.min.js"></script>
    <script>
      // Full list of configuration options available here:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        controls: true,
        progress: true,
        history: true,
        center: true,

        theme: Reveal.getQueryHash().theme, // available themes are in /css/theme
        transition: Reveal.getQueryHash().transition || 'linear', // default/cube/page/concave/zoom/linear/fade/none

        // Parallax scrolling
        // parallaxBackgroundImage: 'https://s3.amazonaws.com/hakim-static/reveal-js/reveal-parallax-1.jpg',
        // parallaxBackgroundSize: '2100px 900px',

        // Optional libraries used to extend on reveal.js
        dependencies: [
          { src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
          { src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
          { src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
          { src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
          { src: 'plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } },
          { src: 'plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } }
        ]
      });
    </script>
    <script type="text/javascript"
      src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>
  </body>
</html>
